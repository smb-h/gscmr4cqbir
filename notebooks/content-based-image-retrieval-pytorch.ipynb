{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Content Based Image Retrieval (CBIR)\n","## Approach:\n","\n","- By observing the data its pretty clear that an Unsupervised alongwith couple of different Hashing approachs will be the most commendable. Although there are number of techniques in that area as well, we'll focus on Hashing and Auto-Encoder techniques:\n","    \n","    - ***Latent Feature Extraction***: In this technique we can find feature vectors for every image by creating hooks on a pre-trained network and extracting the vector from previous layers. Other technique devises the use of **AutoEncoders** where the Latent features can be extracted from Encoder itself. For the sake of this data we'll proceed with AutoEncoders. For the retrieval part we'll look into Euclidean based Search (O(NlogN)) and Hashing Based Approaches (O(logN)).\n","    <br>\n","    - ***Image Hashing Search***: This can be done by:\n","        - Uniquely quantify the contents of an image using only a single integer.\n","        - Find duplicate or near-duplicate images in a dataset of images based on their computed hashes.<br>\n","        <br>\n","      This can be accomplished by a specialized data structure called a **VP-Tree**. Using a VP-Tree we can reduce our search complexity from O(nlogn) to O(log n), enabling us to obtain our sub-linear goal!"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/smbh/.virtualenvs/gscmr4cqbir/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import time\n","import copy\n","import pickle\n","from barbar import Bar\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import cv2\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","from torchvision import transforms\n","from torchsummary import summary\n","\n","# from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","from pathlib import Path\n","import gc\n","import os\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["RANDOMSTATE = 0\n"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# Find if any accelerator is presented, if yes switch device to use CUDA or else use CPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(100, 1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>../data/raw/cbir/images/1457.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>../data/raw/cbir/images/4466.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>../data/raw/cbir/images/3839.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>../data/raw/cbir/images/1101.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>../data/raw/cbir/images/857.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              image\n","0  ../data/raw/cbir/images/1457.jpg\n","1  ../data/raw/cbir/images/4466.jpg\n","2  ../data/raw/cbir/images/3839.jpg\n","3  ../data/raw/cbir/images/1101.jpg\n","4   ../data/raw/cbir/images/857.jpg"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# preparing intermediate DataFrame\n","dataset_path = Path('../data/raw/cbir/images/')\n","df = pd.DataFrame()\n","\n","df['image'] = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n","df['image'] = df['image'].apply(lambda x: dataset_path / x)\n","# filter to first 100 images\n","df = df[:100]\n","\n","print(df.shape)\n","df.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["class CBIRDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        \n","        self.transformations = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    \n","    def __getitem__(self, key):\n","        if isinstance(key, slice):\n","            raise NotImplementedError('slicing is not supported')\n","        \n","        row = self.df.iloc[key]\n","        image = self.transformations(Image.open(row['image']))\n","        return image\n","    \n","    def __len__(self):\n","        return len(self.df.index)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# Intermediate Function to process data from the data retrival class\n","def prepare_data(df):\n","    train_df, validate_df = train_test_split(df, test_size=0.15, random_state=RANDOMSTATE)\n","    train_set = CBIRDataset(train_df)\n","    validate_set = CBIRDataset(validate_df)\n","    \n","    return train_set, validate_set\n"]},{"cell_type":"markdown","metadata":{},"source":["# AutoEncoder Model"]},{"cell_type":"markdown","metadata":{},"source":["## High Level Structure of an AutoEncoder"]},{"cell_type":"markdown","metadata":{},"source":["![](https://hackernoon.com/hn-images/1*op0VO_QK4vMtCnXtmigDhA.png)"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","        # in (N,3,512,512)\n","        self.encoder = nn.Sequential(\n","            # (32, 16, 171, 171)\n","            nn.Conv2d(in_channels=3, \n","                      out_channels=16, \n","                      kernel_size=(3,3), \n","                      stride=3, \n","                      padding=1),  \n","            nn.ReLU(True),\n","            # (N, 16, 85, 85)\n","            nn.MaxPool2d(2, stride=2),  \n","            # (N, 8, 43, 43)\n","            nn.Conv2d(in_channels=16, \n","                      out_channels=8, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1),  \n","            nn.ReLU(True),\n","            # (N, 8, 42, 42)\n","            nn.MaxPool2d(2, stride=1)  \n","        )\n","        self.decoder = nn.Sequential(\n","            # (N, 16, 85, 85)\n","            nn.ConvTranspose2d(in_channels = 8, \n","                               out_channels=16, \n","                               kernel_size=(3,3), \n","                               stride=2),  \n","            nn.ReLU(True),\n","            # (N, 8, 255, 255)\n","            nn.ConvTranspose2d(in_channels=16, \n","                               out_channels=8, \n","                               kernel_size=(5,5), \n","                               stride=3, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            # (N, 3, 512, 512)\n","            nn.ConvTranspose2d(in_channels=8, \n","                               out_channels=3, \n","                               kernel_size=(6,6), \n","                               stride=2, \n","                               padding=1),  \n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["class ConvAutoencoder_v2(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder_v2, self).__init__()\n","        # in (N,3,512,512)\n","        self.encoder = nn.Sequential(\n","            \n","            nn.Conv2d(in_channels=3, \n","                      out_channels=64, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=64, \n","                      out_channels=64, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2), \n","            \n","            nn.Conv2d(in_channels=64, \n","                      out_channels=128, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=128, \n","                      out_channels=128, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=0), \n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2), \n","            \n","            nn.Conv2d(in_channels=128, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=256, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=256, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2) \n","        )\n","        self.decoder = nn.Sequential(\n","            \n","            nn.ConvTranspose2d(in_channels = 256, \n","                               out_channels=256, \n","                               kernel_size=(3,3), \n","                               stride=1,\n","                              padding=1), \n"," \n","            nn.ConvTranspose2d(in_channels=256, \n","                               out_channels=256, \n","                               kernel_size=(3,3), \n","                               stride=1, \n","                               padding=1),  \n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=256, \n","                               out_channels=128, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=0),  \n","            \n","            nn.ConvTranspose2d(in_channels=128, \n","                               out_channels=64, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(in_channels=64, \n","                               out_channels=32, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1), \n","            \n","            nn.ConvTranspose2d(in_channels=32, \n","                               out_channels=32, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            \n","            nn.ConvTranspose2d(in_channels=32, \n","                               out_channels=3, \n","                               kernel_size=(4,4), \n","                               stride=2, \n","                               padding=2),  \n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 512, 512]           1,792\n","              ReLU-2         [-1, 64, 512, 512]               0\n","            Conv2d-3         [-1, 64, 512, 512]          36,928\n","              ReLU-4         [-1, 64, 512, 512]               0\n","         MaxPool2d-5         [-1, 64, 256, 256]               0\n","            Conv2d-6        [-1, 128, 128, 128]          73,856\n","              ReLU-7        [-1, 128, 128, 128]               0\n","            Conv2d-8        [-1, 128, 126, 126]         147,584\n","              ReLU-9        [-1, 128, 126, 126]               0\n","        MaxPool2d-10          [-1, 128, 63, 63]               0\n","           Conv2d-11          [-1, 256, 32, 32]         295,168\n","             ReLU-12          [-1, 256, 32, 32]               0\n","           Conv2d-13          [-1, 256, 32, 32]         590,080\n","             ReLU-14          [-1, 256, 32, 32]               0\n","           Conv2d-15          [-1, 256, 32, 32]         590,080\n","             ReLU-16          [-1, 256, 32, 32]               0\n","        MaxPool2d-17          [-1, 256, 16, 16]               0\n","  ConvTranspose2d-18          [-1, 256, 16, 16]         590,080\n","  ConvTranspose2d-19          [-1, 256, 16, 16]         590,080\n","             ReLU-20          [-1, 256, 16, 16]               0\n","  ConvTranspose2d-21          [-1, 128, 33, 33]         295,040\n","  ConvTranspose2d-22           [-1, 64, 65, 65]          73,792\n","             ReLU-23           [-1, 64, 65, 65]               0\n","  ConvTranspose2d-24         [-1, 32, 129, 129]          18,464\n","  ConvTranspose2d-25         [-1, 32, 257, 257]           9,248\n","             ReLU-26         [-1, 32, 257, 257]               0\n","  ConvTranspose2d-27          [-1, 3, 512, 512]           1,539\n","             Tanh-28          [-1, 3, 512, 512]               0\n","================================================================\n","Total params: 3,313,731\n","Trainable params: 3,313,731\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.00\n","Forward/backward pass size (MB): 678.39\n","Params size (MB): 12.64\n","Estimated Total Size (MB): 694.03\n","----------------------------------------------------------------\n"]}],"source":["summary(ConvAutoencoder_v2().to(device),(3, 512, 512))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training Function"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["def load_ckpt(checkpoint_path, model, optimizer):\n","    \n","    # load check point\n","    checkpoint = torch.load(checkpoint_path)\n","\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    #valid_loss_min = checkpoint['valid_loss_min']\n","\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch']\n","\n","def save_checkpoint(state, filename):\n","    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n","    print (\"=> Saving a new best\")\n","    torch.save(state, filename)  # save checkpoint\n","    \n","def train_model(model,  \n","                criterion, \n","                optimizer, \n","                #scheduler, \n","                num_epochs):\n","    since = time.time()\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = np.inf\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","\n","            # Iterate over data.\n","            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n","                inputs = inputs.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, inputs)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","            #if phase == 'train':\n","            #    scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f}'.format(\n","                phase, epoch_loss))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_loss < best_loss:\n","                best_loss = epoch_loss\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                save_checkpoint(state={   \n","                                    'epoch': epoch,\n","                                    'state_dict': model.state_dict(),\n","                                    'best_loss': best_loss,\n","                                    'optimizer_state_dict':optimizer.state_dict()\n","                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Loss: {:4f}'.format(best_loss))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, optimizer, epoch_loss\n"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# EPOCHS = 150\n","EPOCHS = 15\n","# NUM_BATCHES = 32\n","NUM_BATCHES = 1\n","RETRAIN = False\n","\n","train_set, validate_set = prepare_data(df)\n","\n","dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n","                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n","                }\n","\n","dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n","\n","model = ConvAutoencoder_v2().to(device)\n","\n","criterion = nn.MSELoss()\n","# Observe that all parameters are being optimized\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","# Decay LR by a factor of 0.1 every 7 epochs\n","#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["# If re-training is required:\n","# Load the old model\n","if RETRAIN == True:\n","    # load the saved checkpoint\n","    model, optimizer, start_epoch = load_ckpt('../input/cbirpretrained/conv_autoencoder.pt', model, optimizer)\n","    print('Checkpoint Loaded')\n"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/15\n","----------\n","85/85: [===============================>] - ETA 0.7ss\n","train Loss: 0.1864\n","15/15: [=============================>..] - ETA 0.2s\n","val Loss: 0.1072\n","=> Saving a new best\n","\n","Epoch 1/15\n","----------\n","85/85: [===============================>] - ETA 0.5ss\n","train Loss: 0.0999\n","15/15: [=============================>..] - ETA 0.3s\n","val Loss: 0.0747\n","=> Saving a new best\n","\n","Epoch 2/15\n","----------\n","24/85: [========>.......................] - ETA 43.8s"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, optimizer, loss \u001b[39m=\u001b[39m train_model(model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m      2\u001b[0m                     criterion\u001b[39m=\u001b[39;49mcriterion, \n\u001b[1;32m      3\u001b[0m                     optimizer\u001b[39m=\u001b[39;49moptimizer, \n\u001b[1;32m      4\u001b[0m                     \u001b[39m#scheduler=exp_lr_scheduler,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m                     num_epochs\u001b[39m=\u001b[39;49mEPOCHS)\n","Cell \u001b[0;32mIn[10], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m         loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     62\u001b[0m         optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m \u001b[39m# statistics\u001b[39;00m\n","File \u001b[0;32m~/.virtualenvs/gscmr4cqbir/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n","File \u001b[0;32m~/.virtualenvs/gscmr4cqbir/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model, optimizer, loss = train_model(model=model, \n","                    criterion=criterion, \n","                    optimizer=optimizer, \n","                    #scheduler=exp_lr_scheduler,\n","                    num_epochs=EPOCHS)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the Trained Model\n","torch.save({\n","            'epoch': EPOCHS,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, 'conv_autoencoderv2_200ep.pt')"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Indexing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["transformations = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load Model in Evaluation phase\n","model = ConvAutoencoder_v2().to(device)\n","model.load_state_dict(torch.load('../input/cbirpretrainedv2/conv_autoencoderv2_200ep.pt', map_location=device)['model_state_dict'], strict=False)\n","\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_latent_features(images, transformations):\n","    \n","    latent_features = np.zeros((4738,256,16,16))\n","    #latent_features = np.zeros((4738,8,42,42))\n","    \n","    for i,image in enumerate(tqdm(images)):\n","        tensor = transformations(Image.open(image)).to(device)\n","        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n","        \n","    del tensor\n","    gc.collect()\n","    return latent_features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = df.image.values\n","latent_features = get_latent_features(images, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["indexes = list(range(0, 4738))\n","feature_dict = dict(zip(indexes,latent_features))\n","index_dict = {'indexes':indexes,'features':latent_features}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# write the data dictionary to disk\n","#with open('features.pkl', \"wb\") as f:\n","#    f.write(pickle.dumps(index_dict))"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Image Retrieval "]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"3\"> This will be approached with two ways as discussed in the start:\n","    - Euclidean Search:\n","        - Identifying the Latent Features\n","        - Calculating the Euclidean Distance between them\n","        - Returning the closest N indexes (of images)\n","    \n","    - Locality Sensitive Hashing\n","        - Create hashes of the feature vector from Encoder\n","        - Store it in a Hashing Table\n","        - Identify closest images based on hamming distance"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Euclidean Search Method"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def euclidean(a, b):\n","    # compute and return the euclidean distance between two vectors\n","    return np.linalg.norm(a - b)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def cosine_distance(a,b):\n","    return scipy.spatial.distance.cosine(a, b)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def perform_search(queryFeatures, index, maxResults=64):\n","\n","    results = []\n","\n","    for i in range(0, len(index[\"features\"])):\n","        # compute the euclidean distance between our query features\n","        # and the features for the current image in our index, then\n","        # update our results list with a 2-tuple consisting of the\n","        # computed distance and the index of the image\n","        d = euclidean(queryFeatures, index[\"features\"][i])\n","        results.append((d, i))\n","    \n","    # sort the results and grab the top ones\n","    results = sorted(results)[:maxResults]\n","    # return the list of results\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_montages(image_list, image_shape, montage_shape):\n","\n","    if len(image_shape) != 2:\n","        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n","    if len(montage_shape) != 2:\n","        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n","    image_montages = []\n","    # start with black canvas to draw images onto\n","    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n","                          dtype=np.uint8)\n","    cursor_pos = [0, 0]\n","    start_new_img = False\n","    for img in image_list:\n","        if type(img).__module__ != np.__name__:\n","            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n","        start_new_img = False\n","        img = cv2.resize(img, image_shape)\n","        # draw image to black canvas\n","        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n","        cursor_pos[0] += image_shape[0]  # increment cursor x position\n","        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n","            cursor_pos[1] += image_shape[1]  # increment cursor y position\n","            cursor_pos[0] = 0\n","            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n","                cursor_pos = [0, 0]\n","                image_montages.append(montage_image)\n","                # reset black canvas\n","                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n","                                      dtype=np.uint8)\n","                start_new_img = True\n","    if start_new_img is False:\n","        image_montages.append(montage_image)  # add unfinished montage\n","    return image_montages"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# take the features for the current image, find all similar\n","# images in our dataset, and then initialize our list of result\n","# images\n","fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","queryIdx = 3166# Input Index for which images \n","MAX_RESULTS = 10\n","\n","\n","queryFeatures = latent_features[queryIdx]\n","results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n","imgs = []\n","\n","# loop over the results\n","for (d, j) in results:\n","    img = np.array(Image.open(images[j]))\n","    print(j)\n","    imgs.append(img)\n","\n","# display the query image\n","ax[0].imshow(np.array(Image.open(images[queryIdx])))\n","\n","# build a montage from the results and display it\n","montage = build_montages(imgs, (512, 512), (5, 2))[0]\n","ax[1].imshow(montage)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["testpath = Path('../input/testcbir/Test_Images')\n","testdf = pd.DataFrame()\n","\n","testdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\n","testdf['image'] = '../input/testcbir/Test_Images/' + testdf['image'].astype(str)\n","\n","testdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["testimages = testdf.image.values\n","test_latent_features = get_latent_features(testimages, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_latent_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","MAX_RESULTS = 10\n","queryIdx = 12\n","\n","queryFeatures = test_latent_features[queryIdx]\n","results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n","imgs = []\n","\n","# loop over the results\n","for (d, j) in results:\n","    img = np.array(Image.open(images[j]))\n","    print(j)\n","    imgs.append(img)\n","\n","# display the query image\n","ax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n","\n","# build a montage from the results and display it\n","montage = build_montages(imgs, (512, 512), (5, 2))[0]\n","ax[1].imshow(montage)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 LSHashing Method"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#!pip install lshashpy3"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#from lshashpy3 import LSHash"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Locality Sensitive Hashing\n","# params\n","# k = 12 # hash size\n","# L = 5  # number of tables\n","# d = 14112 # Dimension of Feature vector\n","# lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)\n","\n","# # LSH on all the images\n","# for idx,vec in tqdm(feature_dict.items()):\n","#     lsh.index(vec.flatten(), extra_data=idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## Exporting as pickle\n","#pickle.dump(lsh, open('lsh.p', \"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# def get_similar_item(idx, feature_dict, lsh_variable, n_items=10):\n","#     response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), \n","#                      num_results=n_items+1, distance_func='hamming')\n","    \n","#     imgs = []\n","#     for i in range(1, n_items+1):\n","#         imgs.append(np.array(Image.open(images[response[i][0][1]])))\n","#     return imgs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","# queryIdx = 5\n","\n","# ax[0].imshow(np.array(Image.open(images[queryIdx])))\n","\n","# montage = build_montages(get_similar_item(queryIdx, feature_dict, lsh,10),(512, 512), (5, 2))[0]\n","# ax[1].imshow(montage)"]},{"cell_type":"markdown","metadata":{},"source":["# End Notes\n","\n","- We started with the approach of AutoEncoders for Image Latent Features extraction followed by Image retrieval using Euclidean Distance which was an O(NlogN) approach (Time-Complexity) to Hashing which gave us an ~O(logN) approach\n","\n","- Another approach was to use Hashing on features obtained from SIFT, SURF, OBS and building the VP Trees ans search the images in it."]},{"cell_type":"markdown","metadata":{},"source":["![](https://www.pyimagesearch.com/wp-content/uploads/2019/08/image_hashing_search_engine_steps.png)"]},{"cell_type":"markdown","metadata":{},"source":["# Clustering of Images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans, MiniBatchKMeans\n","from scipy.spatial.distance import cdist\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","\n","import matplotlib.cm as cm\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_latent_features1D(images, transformations):\n","    \n","    latent_features1d = []\n","    \n","    for i,image in enumerate(tqdm(images)):\n","        tensor = transformations(Image.open(image)).to(device)\n","        latent_features1d.append(model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy().flatten())\n","        \n","    del tensor\n","    gc.collect()\n","    return latent_features1d"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = df.image.values\n","latent_features1d = get_latent_features1D(images, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["latent_features1d = np.array(latent_features1d)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["distortions = [] \n","inertias = [] \n","mapping1 = {} \n","mapping2 = {} \n","K = range(4,10) \n","  \n","for k in tqdm(K): \n","    #Building and fitting the model \n","    kmeanModel = KMeans(n_clusters=k).fit(latent_features1d)      \n","      \n","    distortions.append(sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n","                      'euclidean'),axis=1)) / latent_features1d.shape[0]) \n","    inertias.append(kmeanModel.inertia_) \n","  \n","    mapping1[k] = sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n","                 'euclidean'),axis=1)) / latent_features1d.shape[0] \n","    mapping2[k] = kmeanModel.inertia_ "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(K, distortions, 'bx-') \n","plt.xlabel('Values of K') \n","plt.ylabel('Distortion') \n","plt.title('The Elbow Method using Distortion') \n","plt.show() "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X = np.array(latent_features1d)\n","K = range(3,10) \n","\n","for n_clusters in tqdm(K):\n","    # Create a subplot with 1 row and 2 columns\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    fig.set_size_inches(18, 7)\n","\n","    # The 1st subplot is the silhouette plot\n","    # The silhouette coefficient can range from -1, 1 but in this example all\n","    # lie within [-0.1, 1]\n","    ax1.set_xlim([-0.1, 1])\n","    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n","\n","    # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOMSTATE)\n","    cluster_labels = clusterer.fit_predict(X)\n","\n","    # The silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters\n","    silhouette_avg = silhouette_score(X, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters,\n","          \"The average silhouette_score is :\", silhouette_avg)\n","\n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n","\n","    y_lower = 10\n","    for i in range(n_clusters):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = \\\n","            sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                          0, ith_cluster_silhouette_values,\n","                          facecolor=color, edgecolor=color, alpha=0.7)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","    ax1.set_title(\"The silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    # 2nd Plot showing the actual clusters formed\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n","                c=colors, edgecolor='k')\n","\n","    # Labeling the clusters\n","    centers = clusterer.cluster_centers_\n","    # Draw white circles at cluster centers\n","    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n","                c=\"white\", alpha=1, s=200, edgecolor='k')\n","\n","    for i, c in enumerate(centers):\n","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n","                    s=50, edgecolor='k')\n","\n","    ax2.set_title(\"The visualization of the clustered data.\")\n","    ax2.set_xlabel(\"Feature space for the 1st feature\")\n","    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","\n","    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n","                  \"with n_clusters = %d\" % n_clusters),\n","                 fontsize=14, fontweight='bold')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["- The Silhouette score isn't significant for any cluster since its close to 0 for every k, that translates to less differentiability for a point to belong to a particular cluster.\n","- GMM can help in this case because animals share a lot of similar traits with each other in terms of appearance but we have to get the bottleneck case since an animal can only belong to one cluster, so kmeans will be the way to go but a different feature/keypoint detection might help identify right number of clusters."]},{"cell_type":"markdown","metadata":{},"source":["## Using SIFT/SURF/ORB technique"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_dictionary(xfeatures2d, images, n_clusters):\n","    #print('Computing descriptors..')        \n","    desc_list = []\n","    \n","    for image_path in images:\n","        image = cv2.imread(image_path)\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","        kp, dsc = xfeatures2d.detectAndCompute(gray, None)\n","        desc_list.extend(dsc)\n","\n","    desc = np.array(desc_list)\n","    #print('Creating BoW dictionary using K-Means clustering with k={}..'.format(n_clusters))\n","    dictionary = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, verbose=0)\n","    dictionary.fit(desc)\n","    \n","    distortion = sum(np.min(cdist(desc, dictionary.cluster_centers_, \n","                      'euclidean'),axis=1)) / desc.shape[0]\n","    \n","    return distortion"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["orb = cv2.ORB_create()\n","images = df.image.values\n","K = range(4,10)\n","distortions = []\n","\n","for k in tqdm(K):\n","    distortions.append(build_dictionary(orb, images, n_clusters=k))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(K, distortions, 'bx-') \n","plt.xlabel('Values of K') \n","plt.ylabel('Distortion') \n","plt.title('The Elbow Method using Distortion') \n","plt.show() "]},{"cell_type":"markdown","metadata":{},"source":["- The ORB technique tells us there are 6/7 major clusters that are persistent in the data"]}],"metadata":{"kernelspec":{"display_name":"gscmr4cqbir","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"b34b9b54d67f9445fca51535a816c4057b03012273c4b3c57e2776c371907ea6"}}},"nbformat":4,"nbformat_minor":4}
